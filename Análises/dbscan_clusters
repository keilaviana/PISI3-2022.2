import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.decomposition import PCA

# Carregue seus dados, faça o pré-processamento das variáveis categóricas e outras transformações conforme necessário

dataset = pd.read_parquet("tabelafinal.parquet")

dt = dataset[['description','nutrient_name','amount','nutrient_unit']]

# dt['amount'] = dt['amount'].apply(str_to_float)
dt['description'] = dt['description'].apply(lambda x: x.split(',')[0])
dt['description'] = dt['description'].apply(lambda x: x.split('-')[0])
dt['nutrient_name'] = dt['nutrient_name'].apply(lambda x: x.split(' ')[0])
dt['description'] = dt['description'].str.upper()
dt['nutrient_name'] = dt['nutrient_name'].str.upper()
dt['nutrient_unit'] = dt['nutrient_unit'].str.upper()

# Contagem de ocorrências para a coluna 'description'
description_counts = dt['description'].value_counts()

chunk_size = 20  # Número de itens por vez
for i in range(0, len(description_counts), chunk_size):
    chunk = description_counts[i:i + chunk_size]

# Contagem de ocorrências para a coluna 'nutrient_name'
nutrient_name_counts = dt['nutrient_name'].value_counts()

for i in range(0, len(nutrient_name_counts), chunk_size):
    chunk = nutrient_name_counts[i:i + chunk_size]

unit_counts = dt['nutrient_unit'].value_counts()
for i in range(0, len(nutrient_name_counts), chunk_size):
    chunk = unit_counts[i:i + chunk_size]

# Obter os valores que ocorrem menos de 100 vezes
values_to_replace_description = description_counts[description_counts < 200].index
values_to_replace_nutrients = nutrient_name_counts[nutrient_name_counts < 200].index
values_to_replace_unit = unit_counts[unit_counts < 100].index

# Substituir esses valores por 'OUTROS'
dt['description'] = dt['description'].map(lambda x: 'OUTROS' if x in values_to_replace_description else x)
dt['nutrient_name'] = dt['nutrient_name'].map(lambda x: 'OUTROS' if x in values_to_replace_nutrients else x)
dt['nutrient_unit'] = dt['nutrient_unit'].map(lambda x: 'OUTROS' if x in values_to_replace_unit else x)

scaler = StandardScaler()
dt['amount'] = scaler.fit_transform(dt[['amount']])

one_hot_encoder = OneHotEncoder(sparse=False, dtype=int)

encoded_description = one_hot_encoder.fit_transform(dt[['description']])
encoded_nutrient_name = one_hot_encoder.fit_transform(dt[['nutrient_name']])
encoded_nutrient_unit = one_hot_encoder.fit_transform(dt[['nutrient_unit']])

one_hot_encoder = OneHotEncoder(sparse=False, dtype=int)
encoded_description = one_hot_encoder.fit_transform(dt[['description']])
encoded_nutrient_name = one_hot_encoder.fit_transform(dt[['nutrient_name']])
encoded_nutrient_unit = one_hot_encoder.fit_transform(dt[['nutrient_unit']])

encoded_description_df = pd.DataFrame(encoded_description, columns=[f"description_{i}" for i in range(encoded_description.shape[1])])
encoded_nutrient_name_df = pd.DataFrame(encoded_nutrient_name, columns=[f"nutrient_name_{i}" for i in range(encoded_nutrient_name.shape[1])])
encoded_nutrient_unit_df = pd.DataFrame(encoded_nutrient_unit, columns=[f"nutrient_unit_{i}" for i in range(encoded_nutrient_unit.shape[1])])
dt_encoded = pd.concat([dt.drop(['description', 'nutrient_name', 'nutrient_unit'], axis=1),
                        encoded_description_df, encoded_nutrient_name_df, encoded_nutrient_unit_df], axis=1)

dt_encoded.to_parquet('tabela_cod.parquet', index=False)

df = dt_encoded[dt_encoded['amount'] != 0.0]

Q1 = df['amount'].quantile(0.25)
Q3 = df['amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
max_non_outlier = df[df['amount'] <= upper_bound]['amount'].max()
df.loc[df['amount'] > upper_bound, 'amount'] = max_non_outlier

original_data = df.copy()

# Selecionar as colunas relevantes para a clusterização
selected_columns = ['amount'] + [col for col in df.columns if col.startswith('description_') or col.startswith('nutrient_name_') or col.startswith('nutrient_unit_')]
data_for_clustering = df[selected_columns]

# Aplique o DBSCAN
eps = 0.01  # Raio de vizinhança
min_samples = 850 # Número mínimo de amostras em uma vizinhança para formar um cluster
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
cluster_labels = dbscan.fit_predict(data_for_clustering)

# Adicione os rótulos de cluster de volta ao DataFrame original
df['cluster_label'] = cluster_labels

# Visualização dos resultados

# Plotagem do gráfico de silhueta (opcional)
silhouette_avg = silhouette_score(df, cluster_labels)
sample_silhouette_values = silhouette_samples(df, cluster_labels)

plt.figure(figsize=(10, 6))
y_lower = 10
for i in range(len(set(cluster_labels))):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = plt.cm.get_cmap("tab10")(i / len(set(cluster_labels)))  # Usar mapa de cores tab10
    plt.fill_betweenx(range(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    plt.text(-0.1, y_lower + 0.5 * size_cluster_i, f'Cluster {i}', color=color)

    y_lower = y_upper + 10

plt.axvline(x=silhouette_avg, color="red", linestyle="--")
plt.yticks([])
plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
plt.xlabel("Coeficiente de Silhueta")
plt.ylabel("Cluster")
plt.title("Gráfico de Silhueta")
plt.show()

# Redução de dimensionalidade usando PCA (opcional)
pca = PCA(n_components=2)
pca_data = pca.fit_transform(df[selected_columns])
pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])
pca_df['cluster_label'] = df['cluster_label']

# Escolha uma paleta de cores diferente, por exemplo, 'Set1'
color_palette = 'Set1'

# Plotagem da visualização em 2D com a paleta de cores escolhida
plt.figure(figsize=(10, 6))
ax = sns.scatterplot(x='PC1', y='PC2', hue='cluster_label', data=pca_df, palette=color_palette)
plt.title('Agrupamento dos Pontos por Cluster')

ax.legend(loc='upper right', title='Clusters')

plt.xlabel("Primeiro Componente Principal (PC1)")
plt.ylabel("Segundo Componente Principal (PC2)")
plt.show()
 
